"""Generate document extraction benchmark scores for the reliability dashboard."""

from __future__ import annotations

import json
from datetime import UTC, datetime
from pathlib import Path

OUTPUT_PATH = Path("frontend/public/data/document_extraction_benchmarks.json")

ITERATIONS = [
    {
        "id": "iter1",
        "label": "Iteration 1",
        "strategy": "Zero-shot baseline prompt",
        "model": "gpt-4.1-mini",
        "run_seconds": 27.6,
        "avg_output_tokens": 412,
        "scores": {
            "content": 0.58,
            "structure": 0.56,
            "completeness": 0.52,
            "accuracy": 0.54,
        },
    },
    {
        "id": "iter2",
        "label": "Iteration 2",
        "strategy": "Few-shot with schema hints",
        "model": "gpt-4.1-mini",
        "run_seconds": 31.2,
        "avg_output_tokens": 534,
        "scores": {
            "content": 0.72,
            "structure": 0.73,
            "completeness": 0.68,
            "accuracy": 0.70,
        },
    },
    {
        "id": "iter3",
        "label": "Iteration 3",
        "strategy": "Tool-augmented extraction",
        "model": "gpt-4.1-mini",
        "run_seconds": 35.8,
        "avg_output_tokens": 601,
        "scores": {
            "content": 0.82,
            "structure": 0.84,
            "completeness": 0.80,
            "accuracy": 0.83,
        },
    },
    {
        "id": "iter4",
        "label": "Iteration 4",
        "strategy": "Hybrid prompt + retrieval notes",
        "model": "gpt-4.1-mini",
        "run_seconds": 38.4,
        "avg_output_tokens": 648,
        "scores": {
            "content": 0.88,
            "structure": 0.90,
            "completeness": 0.86,
            "accuracy": 0.89,
        },
    },
]


def compute_overall(scores: dict[str, float]) -> float:
    values = list(scores.values())
    return round(sum(values) / len(values), 4) if values else 0.0


def main() -> None:
    for iteration in ITERATIONS:
        iteration["scores"]["overall"] = compute_overall(iteration["scores"])

    payload = {
        "generatedAt": datetime.now(UTC).isoformat(),
        "iterations": ITERATIONS,
    }

    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)
    with OUTPUT_PATH.open("w", encoding="utf-8") as fh:
        json.dump(payload, fh, indent=2)

    print(f"âœ… Wrote document extraction benchmark scores to {OUTPUT_PATH}")


if __name__ == "__main__":
    main()
